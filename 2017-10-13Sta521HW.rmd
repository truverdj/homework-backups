---
title: 'HW4: Team 6'
author: '[Chong Chen, Daniel Truver, Matt Welch]'
date: "Due October 13, 2017"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---


```{r setup, echo=FALSE}
suppressMessages(library(ISLR))
suppressMessages(library(arm))
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages
```
<!--
This problem set has several dependent parts, so plan accordingly.  Here is a suggested outline to finish the assignment on time:

* Start problems 1-2, 4, 7 and 10 prior to Monday individually and with your group adding working code and minimal documentation for now.  It is important to get a head start on the model building.

* Try problem 3 and 6 prior to lab Wednesday so that you will be prepared to ask questions about simulation and coding with the goal of having a minimal working version for 3 and 6 by the end of lab.  This will help with the later questions where you apply it to the other models.  (work as much on those as well)

* don't forget midterm and take time to enjoy fall break

* Try problem 12, 14 and 15 before Lab on the 11th; use lab time to refine code, ask questions about interpretation, theory etc.  

* finish write up and turn in on Sakai on the 13th.  Please let us know if there are problems with missing packages for wercker as you go so that you have a passing badge.  Remove any instructions like this and above to clean up the presentation.
--->

## Preliminaries
<!--
Load the college application data from Lab1 and create the variable `Elite` by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50 %.  We will also save the College names as a new variable and remove `Accept` and `Enroll` as temporally they occur after applying, and do not make sense as predictors in future data.
--->

```{r data}
data(College)
College = College %>% 
  mutate(college = rownames(College)) %>%
  mutate(Elite = factor(Top10perc > 50)) %>%
  mutate(Elite = 
           recode(Elite, 'TRUE' = "Yes", 'FALSE'="No")) %>%
  select(c(-Accept, -Enroll))
```
<!--
We are going to create a training and test set by randomly splitting the data.  First set a random seed by
--->
```{r setseed}
# do not change this; for a break google `8675309`
set.seed(8675309)
n = nrow(College)
n.train = floor(.75*n)
train = sample(1:n, size=n.train, replace=FALSE)
College.train = College[train,]
College.test = College[-train,]
```


<!--
1. Create scatter plots of predictors versus `Apps` using the training data only.  If you use pairs or preferably `ggpairs` make sure that `Apps` is on the y-axis in plots versus the other predictors.  (Make sure that the plots are legible, which may require multiple plots.)  
Comment on any features in the plots, such as potential outliers, non-linearity, needs for transformations etc.
--->
```{r scatterplots1}
pairs( College.train %>% 
        select(Apps, Top10perc, Top25perc, F.Undergrad, P.Undergrad))
```

Were we to perform a simple regression of apps with any of the above variables, it appears we may be dealing with non-constant variance, but no obvious non-linear patter with 'Top10perc,' 'Top25Perc,' or 'F.Undergrad.' We may want to do a long transform on 'P.Undergrad.' This transformation will also help with the potential outlier school that has a massive (> 20,000) part-time undergraduate population.  


```{r scatterplots2}
pairs( College.train %>% 
        select(Apps, Outstate, Room.Board, Personal, PhD))
```

The relationship between 'PhD' and 'Apps' appears exponential. The variables 'Outstate,' 'Room.Board,' and 'Peronsal' do not show very distinct patterns, linear or otherwise, in relation to 'Apps.'

```{r scatterplots3}
pairs( College.train %>% 
        select(Apps, S.F.Ratio, Expend, Grad.Rate, Elite))
```

Here, we see would could be a log-like relationship between 'Expend' and 'Apps' as well as what appears to be a linear without constant variance relationship between 'Grad.Rate' and 'Apps.' 'S.F.Ratio' and 'Elite' do not present any obvious relationships with 'Apps.'  

<!--
2.  Build a linear regression model to predict `Apps` from the other predictors using the training data.  Present model summaries and diagnostic plots.   Based on diagnostic plots  using residuals,  comment on the  adequacy of your model.
--->
```{r Model1st}
College.train = College.train %>%
  mutate(Elite = recode(Elite, "Yes" = 1, "No" = 0)) %>%
  mutate(Private = recode(Private, "Yes" = 1, "No" = 0)) %>%
  select(-college)
College.test = College.test %>%
  mutate(Elite = recode(Elite, "Yes" = 1, "No" = 0)) %>%
  mutate(Private = recode(Private, "Yes" = 1, "No" = 0)) %>%
  select(-college)
apps.lm2 = lm(data = College.train, Apps ~ .)
summary(apps.lm2)
```

```{r Model1stPlots}
{
  par(mfrow = c(2,2))
  plot(apps.lm2)
}
```

We do not see any troublesome points from a look at Cook's Distance. We do notice a few poins with large standarized residuals and high leverage. The residual plot shows a strange, almost non-linear patter for small fitted values. We can also see from this plot that the variane is defnitely not constant. The residual plot should let us know quickly that there is something wrong with the model. The Normal Q-Q plot reinforces this suspicion; points lie much farther away from the mean than we would expect. 

<!--
3. Generate 1000 replicate data sets using the coefficients from the model you fit above.  Using RMSE as a statistic, $\sqrt{\sum_i(y^{\text{rep}} - \hat{y}_i^{\text{rep}})^2/n }$, how does the RMSE from the model based on the training data compare to RMSE's based on the replicated data.  What does this suggest about model adequacy?   Provide a histogram of the RMSE's with a line showing the location of the observed RMSE and compute a p-value.  Hint:  write a function to calculate RMSE.
--->

```{r predictiveChecks1}
rmse = function(y, ypred){
       rmse = sqrt(mean((y - ypred)^2))
       return(rmse)
}
set.seed(1)
nsim = 1000
n = nrow(College.train)
appsModel = model.matrix(apps.lm2)
sim.apps.lm2 = sim(apps.lm2, nsim)
rmseList = rep(NA, nsim)
rmseTrain = rmse(College.train$Apps, apps.lm2$fitted.values)
for (i in 1:nsim){
  ypred_i = appsModel %*% sim.apps.lm2@coef[i,]
  y.rep = rnorm(length(ypred_i), ypred_i, sim.apps.lm2@sigma[i])
  sim.lm = lm(data = data.frame(y.rep, appsModel[,-1]), y.rep ~.)
  rmseList[i] = rmse(y.rep, sim.lm$fitted.values)
}
p.value1 = mean(rmseList > rmseTrain)
ggplot(data = data.frame(rmseList), aes(x = rmseList)) +
  geom_histogram(color = "gray50", fill = "gray75") + 
  geom_vline(xintercept = rmseTrain, color = "red") +
  geom_text(aes(x = rmseTrain, y = 50, 
                label = "model RMSE \n p-value = 0.471"),
            hjust = 0.2) +
  theme_bw()
```

<!--
4. Build a second model, considering transformations of the response and predictors, possible interactions, etc with the goal of trying to achieve  a model where assumptions for linear regression are satisfied, providing justification for your choices.
Comment on  how well the assumptions are met and and issues that diagnostic plots may reveal.
--->

The first things first, we're going to do is remove 'Books' and 'PhD' as predictors. They are statistically insignificant and a positive coefficient for the cost of books makes no sense. Additionally, the maximum value for in the data for 'PhD', the percent of PhD faculty, is 103. A negative coefficient on 'perc.alumni,' the percent of alumni who donate seems a bit strange; perhaps smaller schools require more donations. We leave it in for now.  

We will log transform 'Expend' since it has a wide range (3365 to 56233), and our original scatterplot showed a possible log-like pattern with 'Apps.' Likewise, as we mentioned before, we will try a log trnasform on 'P.Undergrad.' Following the intuitive rule that we should explore interaction for large magnitude coefficients, we will explore the interaction of 'Private' with 'Elite' and 'Grad.Rate' with 'Elite.'

```{r model2nd}
apps.try.lm4 = lm(data = College.train,
              Apps ~ Private + Top10perc + Top25perc + F.Undergrad + log(P.Undergrad) +
                Room.Board + Outstate + Terminal + S.F.Ratio + perc.alumni + log(Expend) +
                Grad.Rate + Elite + Private*Elite + Grad.Rate*Elite)
{
  par(mfrow = c(2,2))
  plot(apps.try.lm4)
}
```

Oh look, everything is still horrible. Most notably, the residuals clearly do not have constant variance. We have managed to reduce leverage values, as was our aim with the log transforms. 

```{r}
apps.lm4 = lm(data = College.train,
              Apps^(1/3) ~ Private + Top10perc + Top25perc + F.Undergrad + log(P.Undergrad) +
                Room.Board + Outstate + Terminal + S.F.Ratio + perc.alumni + log(Expend) +
                Grad.Rate + Elite + Private*Elite + Grad.Rate*Elite)
{
  par(mfrow = c(2,2))
  plot(apps.lm4)
}
```

Trying a few of our favorite power transforms on the response, specifically $\lambda \in \{-1,-1/2,-1/3,0,1/3,1/2,1\} $, we see that 1/2 and 1/3 make decent improvements on the residual variability problem, but they do little to improve the strange non-linearity we see in the residuals. 

<!--
5.  Repeat the predictive checks described in problem 3, but using your model from problem 4.  If you transform the response, you will need to back transform  data to the original units in order to compute the RMSE in the original units.  Does this suggest that the model is adequate?  Do the two graphs provide information about which model is better?
--->

```{r predictiveChecks2, cache=T}
set.seed(1)
nsim = 1000
appsModel2 = model.matrix(apps.lm4)
sim.apps.lm4 = sim(apps.lm4, nsim)
rmseList2 = rep(NA, nsim)
rmseTrain2 = rmse(College.train$Apps, (apps.lm4$fitted.values)^3)
for (i in 1:nsim){
  ypred_i = (appsModel2 %*% sim.apps.lm4@coef[i,])^3
  y.rep = rnorm(length(ypred_i), ypred_i, sim.apps.lm4@sigma[i])
  sim.lm = lm(data = data.frame(y.rep, appsModel2[,-1]), y.rep ~.)
  rmseList2[i] = rmse(y.rep, sim.lm$fitted.values)
}
ggplot(data = data.frame(rmseList2), aes(x = rmseList2)) +
  geom_histogram(color = "gray50", fill = "gray75") + 
  geom_vline(xintercept = rmseTrain2, color = "red") +
  theme_bw()
```

That actually looks much worse than the original. The p-value here is 0.   

<!---
6. Use your two fitted models to predict the number of applications for the testing data, `College.test`.  Plot the predicted residuals $y_i - \hat{y}_i$  versus the predictions.  Are there any cases where the model does a poor job of predicting?  Compute the RMSE using the test data
where now RMSE = $\sqrt{\sum_{i = 1}^{n.test}(y_i - \hat{y}_i)^2/n.test}$ where the sum is over the test data.  Which model is better for the out of sample prediction?
--->

```{r}
College.test.pred = College.test
pred1 = predict(apps.lm2, newdata = College.test.pred, type = "response")
pred1.resid.df = data.frame(predictions = pred1, residuals = College.test$Apps - pred1) 
ggplot(data = pred1.resid.df, aes(x = predictions, y = residuals)) +
  theme_bw() + 
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dotted")
```

Oh boy, non-linearity AND non-constant variance. It's like Christmas for the emobodiment of statisticians' suffering.

```{r prediction2}
pred2 = predict(apps.lm4, newdata = College.test.pred, type = "response")
pred2.resid.df = data.frame(predictions = pred2^3, residuals = College.test$Apps - pred2^3) 
ggplot(data = pred2.resid.df, aes(x = predictions, y = residuals)) +
  theme_bw() + 
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dotted")
```

The non-constant variance is still around, we still have so non-linearity on the right and that non-constant variance. 

```{r}
out.sample.1 = rmse(College.test$Apps ,pred1)
in.sample.1 = rmseTrain
out.sample.2 = rmse(College.test$Apps ,pred2)
in.sample.2 = rmseTrain2
df = data.frame(Out.of.Sample = c(out.sample.1, out.sample.2),
                Observed = c(in.sample.1, in.sample.2), 
                row.names = c("RMSE1", "RMSE2"))
kable(df)
```

Our first model appears to be better for out of sample prediction.

<!--
7.  As the number of applications is a count variable, a Poisson regression model is a natural alternative for modelling this data.   Build a Poisson model using  main effects and possible interactions/transformations.    Comment on the model adequacy based on diagnostic plots and other summaries. Is there evidence that there is lack of fit?
--->

```{r poissonModel1}
apps.try.glm7 = glm(data = College.train, Apps ~ ., family = poisson(link = "log"))
summary(apps.try.glm7)
{
  par(mfrow = c(2,2))
  plot(apps.try.glm7)
}
```

Well, the residuals look much better, but the Cook's distance is now going to space. Additionally, we have a residual deviacne of 386292 on 565 degrees of freedom. Let's try that a bit differently.

```{r}
apps.try.glm7 = glm(data = College.train,
                    Apps ~ Private + Top10perc + Top25perc + F.Undergrad + log(P.Undergrad) +
                  Room.Board + Outstate + Terminal + S.F.Ratio + perc.alumni + log(Expend) +
                  Grad.Rate + Elite + Private*Elite + Grad.Rate*Elite, 
                  family = poisson(link = "log"))
summary(apps.try.glm7)
{
  par(mfrow = c(2,2))
  plot(apps.try.glm7)
}
```

Well, the residual deviance has come down to 377990 on 566 degrees of freedom. So far, both models have shown significant signs of overdispersion. We'll try one more thing, mostly to address our leverage problems, log transfomations on 'F.Undergrad' and 'Outstate.' We also finally remove 'perc.alumni' from the model. 

```{r}
apps.glm7 = glm(data = College.train,
                    Apps ~ Private + Top10perc + Top25perc + log(F.Undergrad) + log(P.Undergrad) +
                  Room.Board + log(Outstate) + Terminal + S.F.Ratio + log(Expend) +
                  Grad.Rate + Elite + Private*Elite + Grad.Rate*Elite, 
                  family = poisson(link = "log"))
summary(apps.glm7)
{
  par(mfrow = c(2,2))
  plot(apps.glm7)
}
```

Our residual deviance has come down again, but only to 173511 on 567 degrees of freedom. However, even with this compensation, the residuals still grow too large to control the Cook's distance. We can now clearly see a pattern of increasing variance in the residuals with less non-linearity, so we move forward with this model. We'll keep an eye on points '76,' '370,' and '279' as we move forward. All together, we have an overdispersion probelm with even this last model possessing an estimate of overdispersion at 306.02

<!--
8.  Generate 1000 replicate data sets using the coefficients from the Poisson model you fit above.  Using RMSE as a statistic, $\sqrt{\sum_i(y^{\text{rep}} - \hat{y}_i^{\text{rep}})^2/n }$, how does the RMSE from the model based on the training data compare to RMSE's based on the replicated data.  What does this suggest about model adequacy?   Provide a histogram of the RMSE's with a line showing the location of the observed RMSE and compute a p-value.
--->

```{r, cache=T}
set.seed(1)
nsim = 1000
appsModel7 = model.matrix(apps.glm7)
sim.apps.glm7 = sim(apps.glm7, nsim)
rmseList7 = rep(NA, nsim)
rmseTrain7 = rmse(College.train$Apps, apps.glm7$fitted.values)
for (i in 1:nsim){
  ypred_i = exp((appsModel7 %*% sim.apps.glm7@coef[i,]))
  y.rep = rpois(length(ypred_i), ypred_i)
  sim.lm = glm(data = data.frame(y.rep, appsModel[,-1]), y.rep ~., family = poisson(link = "log"))
  rmseList7[i] = rmse(y.rep, sim.lm$fitted.values)
}
ggplot(data = data.frame(rmseList7), aes(x = rmseList7)) +
  geom_histogram(color = "gray50", fill = "gray75") + 
  geom_vline(xintercept = rmseTrain7, color = "red") +
  theme_bw()
```

The p-value here is 1.

<!--
9.  Using the test data set, calculate the RMSE for the test data using the predictions from the Poisson model.  How does this compare to the RMSE based on the observed data?  Is this model better than the linear regression models in terms of out of sample prediction?
--->

```{r}
glm7.pred = predict(apps.glm7, newdata = College.test, type = "response")
glm7.pred.rmse = rmse(College.test$Apps, glm7.pred)
glm7.obs.rmse = rmseTrain7
df = data.frame(Out.of.Sample = glm7.pred.rmse, Observed = glm7.obs.rmse, row.names = "RMSE")
kable(df)
```

This model does have a lower RMSE out of sample than the two previous linear models. In that way, it is better than said models. 

<!--
10. Build a model using the negative binomial model (consider transformations and interactions if needed) and examine diagnostic plots.  Are there any suggestions of problems with this model?
--->

```{r}
apps.glm.nb = glm.nb(data = College.train,
                    Apps ~ Private + Top10perc + Top25perc + log(F.Undergrad) + log(P.Undergrad) +
                  Room.Board + log(Outstate) + Terminal + S.F.Ratio + log(Expend) +
                  Grad.Rate + Elite + Private*Elite + Grad.Rate*Elite)
summary(apps.glm.nb)
{
  par(mfrow = c(2,2))
  plot(apps.glm.nb)
}
```

The diagnostics and residual deviance (596.48 on 567 degrees of freedom) do look much better; our estimate of overdispersion is only 1.05. The residuals lie nicely between -4 and 4, with only a few of the problematic points we say earlier, namely '76' and '370' standing out from the crowd. Cook's distance and leverage are also back to reasonable levels.

<!--
11. Carry out the predictive checks for the negative binomial model model using simulated replicates with RMSE and add RMSE from the test data and observed data to your plot.  What do these suggest about 1) model adequacy and 2) model comparison?  Which model out of all that you have fit do you recommend?  
--->

```{r, cache = T}
set.seed(1)
nsim = 1000
appsModel.nb = model.matrix(apps.glm.nb)
class(apps.glm.nb) = "glm"
sim.apps.glm.nb = sim(apps.glm.nb, nsim)
rmseList.nb = rep(NA, nsim)
rmseTrain.nb = rmse(College.train$Apps, apps.glm.nb$fitted.values)
sim.apps.glm.nb@sigma = rnorm(nsim, apps.glm.nb$theta, apps.glm.nb$SE.theta)
for (i in 1:nsim){
  ypred_i = exp(appsModel.nb %*% sim.apps.glm.nb@coef[i,])
  y.rep = rnegbin(length(ypred_i), mu = ypred_i, theta = sim.apps.glm.nb@sigma[i])
  sim.lm = glm.nb(data = data.frame(y.rep, appsModel[,-1]), y.rep ~.)
  rmseList.nb[i] = rmse(y.rep, sim.lm$fitted.values)
}
pred.nb = predict(apps.glm.nb, newdata = College.test, type = "response")
rmseTest.nb = rmse(College.test$Apps, pred.nb)
ggplot(data = data.frame(rmseList.nb), aes(x = rmseList.nb)) +
  geom_histogram(color = "gray50", fill = "gray75") + 
  geom_vline(xintercept = rmseTrain.nb, color = "red") +
  geom_vline(xintercept = rmseTest.nb, color = "blue") +
  geom_text(aes(x = rmseTrain.nb, y = 100,
                label = "Training\n RMSE")) +
  geom_text(aes(x = rmseTest.nb, y = 100, 
                label = "Test\nRMSE")) +
  theme_bw()
```

Our p-value is, once again, equal to 1. 

<!--
12.  While RMSE is a popular summary for model goodness of fit, coverage of confidence intervals is an alternative. For each case in the test set, find a 95% prediction interval.  Now evaluate if the response is in the test data are inside or outside of the intervals.   If we have the correct coverage, we would expect that at least 95\% of the intervals would contain the test cases. Write a function to calculate coverage (the input should be the fitted model object and the test data-frame) and then evaluate coverage for each of the  models that you fit  (the two normal, the  Poisson and the negative binomial).  Include plots of the confidence intervals versus case number ordered by the prediction, with the left out data added as points.  Comment on the plots, highlighting any unusual colleges where the model predicts poorly.
--->

```{r}
coverage = function(intervals, data){ # intervals should be a predict object with intervals
  is.in.interval = (intervals[,2] < data & data < intervals[,3])
  percCoverage = mean(is.in.interval)
  return(percCoverage)
}
# idea to do this "bootstrap" method
# https://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression
theta1 = apps.glm.nb$theta
se.theta1 = apps.glm.nb$SE.theta
glmCoverage = function(model, modelMatrix, nsim, newResponse, is.simple = TRUE, is.pois = TRUE){ 
  # hold onto your butts; it's about to get empirical 
  set.seed(1)
  class(model) = "glm"
  # modelMatrix = model.matrix(apps.glm7, data = College.test)
  simMatrix = sim(model, nsim)
  # if (is.pois == FALSE){
  #   simMatrix@sigma = rnorm(nsim, 
  #                           mean = model["theta"], 
  #                           sd = model["SE.theta"])
  # }
  tempMatrix = matrix(NA, ncol = nsim, nrow = length(newResponse))
  for (j in 1:nsim){
    if (is.simple == T){
      y_pred =exp(modelMatrix %*% simMatrix@coef[j,])
    } else if (is.pois == T) {
      y_pred = rpois(length(newResponse), exp(modelMatrix %*% simMatrix@coef[j,]))
    } else {
      y_pred = rnegbin(length(newResponse), mu = exp(modelMatrix %*% simMatrix@coef[j,]), 
                       theta = rnorm(1, theta1, se.theta1))
    }
    tempMatrix[,j] = y_pred
  }
  intervalMatrix = matrix(NA, ncol = 2, nrow = length(newResponse))
  for (i in 1:length(newResponse)){
    lwr = min(tempMatrix[i,])
    upr = max(tempMatrix[i,])
    intervalMatrix[i, 1] = lwr
    intervalMatrix[i, 2] = upr
  }
  is.in.interval = (intervalMatrix[,1] < newResponse & newResponse < intervalMatrix[,2])
  percCoverage = mean(is.in.interval)
  return(data.frame(cov = percCoverage, int = intervalMatrix))
}
normal1.pred = predict(apps.lm2, newdata = College.test, interval = "predict")
normal1.coverage = coverage(normal1.pred, College.test$Apps)
normal2.pred = predict(apps.lm4, newdata = College.test, interval = "predict")
normal2.coverage = coverage(normal2.pred, College.test$Apps^(1/3))
# https://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression
modelMatrix.pois = model.matrix(apps.glm7, data = College.test)
poisson.coverage.df = glmCoverage(model = apps.glm7, modelMatrix = modelMatrix.pois,
                               nsim = 1000, newResponse = College.test$Apps, is.simple = F)
poisson.coverage = poisson.coverage.df[1,1]
poisson.coverage.df = poisson.coverage.df[,-1]
# model.matrix will not cooperate here with the actual glm.nb object, 
# so we have to use a fairly stupid workaround
apps.nb = Apps ~ Private + Top10perc + Top25perc + log(F.Undergrad) + log(P.Undergrad) +
                  Room.Board + log(Outstate) + Terminal + S.F.Ratio + log(Expend) +
                  Grad.Rate + Elite + Private*Elite + Grad.Rate*Elite
modelMatrix.nb = model.matrix(apps.nb, data = College.test)
neg.bin.coverage.df = glmCoverage(model = apps.glm.nb, modelMatrix = modelMatrix.nb,
                               nsim = 1000, newResponse = College.test$Apps, 
                               is.simple = F, is.pois = F)
neg.bin.coverage = neg.bin.coverage.df[1,1]
neg.bin.coverage.df = neg.bin.coverage.df[,-1]
coverage.df = data.frame(Normal.Coverage_1 = normal1.coverage,
                Normal.Coverage_2 = normal2.coverage,
                Poisson.Coverage = poisson.coverage,
                Negative.Binomial.Coverage = neg.bin.coverage)
coverage.c = c(normal1.coverage, normal2.coverage, poisson.coverage, neg.bin.coverage)
kable(coverage.df)
```

 Well, the poisson turns out to actaully have the worst coverage. We would expect this from our previous analysis of its overdispersion. The normal models hold up surprisingly well given their poor performance with previous diagnostics. The negative binomial has the best coverage here and appears the best of the models that we have created.  

```{r, echo=F}
fit = predict(apps.lm2, newdata = College.test, type = "response")
df = data.frame(obs = College.test$Apps,
                normal1.pred)
df = df %>%
  arrange(lwr)
df = data.frame(collegeNum = 1:length(College.test$Apps), df)
ggplot(data = df, aes(x = collegeNum)) +
  theme_bw() +
  geom_point(aes(x = collegeNum, y = fit), pch = 3) + 
  geom_errorbar(aes(ymin = lwr, ymax = upr)) +
  geom_point(aes(x = collegeNum, y = obs), pch = 4) +
  ggtitle("1st Normal Prediction Intervals")
```

```{r, echo=F}
fit = predict(apps.lm4, newdata = College.test, type = "response")
df = data.frame(obs = College.test$Apps,
                (normal2.pred)^3)
df = df %>%
  arrange(lwr)
df = data.frame(collegeNum = 1:length(College.test$Apps), df)
ggplot(data = df, aes(x = collegeNum)) +
  theme_bw() +
  geom_point(aes(x = collegeNum, y = fit), pch = 3) + 
  geom_errorbar(aes(ymin = lwr, ymax = upr)) +
  geom_point(aes(x = collegeNum, y = obs), pch = 4) +
  ggtitle("2nd Normal Prediction Intervals")
```
 
```{r, echo=F}
fit = predict(apps.glm7, newdata = College.test, type = "response")
df = data.frame(fit = fit,
                obs = College.test$Apps,
                poisson.coverage.df)
df = df %>%
  arrange(int.2)
df = data.frame(collegeNum = 1:length(College.test$Apps), df)
ggplot(data = df, aes(x = collegeNum)) +
  theme_bw() +
  geom_point(aes(x = collegeNum, y = fit), pch = 3) + 
  geom_errorbar(aes(ymin = int.1, ymax = int.2)) +
  geom_point(aes(x = collegeNum, y = obs), pch = 4) +
  ggtitle("Poisson Prediction Intervals")
```

```{r, echo=F}
fit = predict(apps.glm.nb, newdata = College.test, type = "response")
df = data.frame(fit = fit,
                obs = College.test$Apps,
                neg.bin.coverage.df)
df = df %>%
  arrange(int.2)
df = data.frame(collegeNum = 1:length(College.test$Apps), df)
ggplot(data = df, aes(x = collegeNum)) +
  theme_bw() +
  geom_errorbar(aes(ymin = int.1, ymax = int.2)) +
  geom_point(aes(x = collegeNum, y = obs), pch = 4) +
  ggtitle("Negative Binomial Prediction Intervals")
```

<!--
13.  Provide a table  with 
the 1) RMSE's on the observed data, 2) RMSE's on the test data, 3) coverage, 4) the predictive check p-value with one row for each of the  models and comment the results.  Which model do you think is best and why?  Consider the job of an administrator who wants to ensure that there are enough staff to handle reviewing applications.  Explain why coverage might be useful.
--->

```{r}
modelList = list(apps.lm2, apps.lm4, apps.glm7, apps.glm.nb)
rmseTest.list = rep(NA, 4)
for (i in 1:4){
  ypred = predict(modelList[[i]], newdata = College.test, type = "response")
  rmseTest.list[i] = rmse(College.test$Apps, ypred)
}
table.df = data.frame(rmseObs = c(rmseTrain, rmseTrain2, rmseTrain.nb, rmseTrain.nb),
                      rmseTest = rmseTest.list,
                      coverage = c(normal1.coverage, normal2.coverage, poisson.coverage, neg.bin.coverage),
                      p.values = c(p.value1, 0, 1, 1),
                      row.names = c("Model1", "Model2", "Model3", "Model4"))
kable(table.df)
```

<!--
14.  For your "best" model  provide a nicely formatted table (use `kable()` or `xtable()`) of relative risks and 95% confidence intervals.  Pick 5 of the most important variables and provide a paragraph that provides an interpretation of the parameters (and intervals) that can be provided to a university admissions officer about which variables increase admissions.  
--->

```{r}
RR = exp(coef(apps.glm.nb))
lwrConf = exp(coef(apps.glm.nb) - 1.96 * coef(summary(apps.glm.nb))[, "Std. Error"])
uprConf = exp(coef(apps.glm.nb) + 1.96 * coef(summary(apps.glm.nb))[, "Std. Error"])
risk.df = data.frame(RR, lwrConf, uprConf)
colnames(risk.df) = c("RR", "2.5%", "97.5%")
kable(risk.df)
```

We have found that one of the most important variables to be the undergraduate student body size. This leads to the somewhat trivial result that if the school wants to increase admissions they should admit more people. Groundbreaking stuff here. On the other hand, elite institutions seem to face lower numbers of applications; lower the standards. Or maybe it is an attitude problem. Our model also shows that increasing instructional expenditure will lead to more applications. Perhaps students like to know that they will receive quality instruction in the classroom. We will advise the admissions office of this grand revelation. Raising out of state tuition also leads to more applications by our model. No need to inform the admissions department of this one, seems they have been employing this technique for years. Finally, we note that being both a private and an elite institution has a negative effect on applications, but this seems to be mostly due to the elite aspect, not the private status.

### Some Theory   


15. Gamma mixtures of Poissons:  From class we said that
\[
\begin{align}
Y \mid \lambda & \sim P(\lambda) \\
p(y \mid \lambda) & = \frac{\lambda^y e^{-\lambda}} {y!} \\
& \\
\lambda \mid \mu, \theta & \sim G(\theta, \theta/\mu)  \\
p(\lambda \mid  \mu, \theta) & = \frac{(\theta/ \mu)^\theta}{\Gamma(\theta)} \lambda^{\theta - 1} e^{- \lambda \theta/\mu} \\
& \\
p(Y \mid \mu, \theta) & = \int p(Y \mid \lambda) p(\lambda \mid \theta, \theta/\mu) d \lambda \\
 & =   \frac{ \Gamma(y + \theta)}{y! \Gamma(\theta)}
\left(\frac{\theta}{\theta + \mu}\right)^{\theta}
\left(\frac{\mu}{\theta + \mu}\right)^{y} \\
Y \mid \mu, \theta & \sim NB(\mu, \theta) 
\end{align}
\]
Derive the density of $Y \mid \mu, \theta$ in (8) showing your work using LaTeX expressions.  (Note this may not display if the output format is html, so please use pdf.)
Using iterated expectations with the Gamma-Poisson mixture, find the mean and variance of $Y$, showing your work.

